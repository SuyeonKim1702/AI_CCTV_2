{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after cnn1  (?, 48, 48, 16)\n",
      "shape after cnn2 : (?, 24, 24, 32)\n",
      "## FLAGS.stride1  1\n",
      "shape after cnn3 : (?, 12, 12, 64)\n",
      "shape after cnn4 : (?, 6, 6, 128)\n",
      "shape after fc1 : (?, 256)\n",
      "shape after fc2 : (?, 256)\n",
      "WARNING:tensorflow:From <ipython-input-1-4309cef58954>:259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "shape after dropout : (?, 256)\n",
      "shape after final layer : (?, 3)\n",
      "INFO:tensorflow:Restoring parameters from face_recognition/face_recog\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import io\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "\n",
    "learning_rate= 0.001\n",
    "image_size= 96\n",
    "image_color= 3\n",
    "batch_size=100\n",
    "num_classes=3\n",
    "maxpool_filter_size= 2\n",
    "log_dir='C:\\\\Users\\\\HEAJIN\\\\' \n",
    "\n",
    "#conv_1\n",
    "conv1_filter_size = 3\n",
    "conv1_layer_size = 16\n",
    "stride1 = 1\n",
    "\n",
    "#conv_2\n",
    "conv2_filter_size = 3\n",
    "conv2_layer_size = 32\n",
    "stride2 = 1\n",
    "\n",
    "#conv_3\n",
    "conv3_filter_size = 3\n",
    "conv3_layer_size = 64\n",
    "stride3 = 1\n",
    "\n",
    "#conv_4\n",
    "conv4_filter_size = 5\n",
    "conv4_layer_size = 128\n",
    "stride4 = 1\n",
    "\n",
    "#fc_1\n",
    "input_layer_size = 6*6*conv4_layer_size\n",
    "fc1_layer_size = 256\n",
    "\n",
    "#fc_2\n",
    "fc2_layer_size = 256\n",
    "\n",
    "W_fo = tf.Variable(tf.truncated_normal([fc2_layer_size,num_classes],stddev=0.1))\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([fc1_layer_size,fc2_layer_size],stddev=0.1))\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([input_layer_size,fc1_layer_size],stddev=0.1))\n",
    "l2reg = 0.01 * tf.reduce_sum(tf.square(W_fo))\n",
    "\n",
    "def get_input_queue(csv_file_name,num_epochs = None):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    for line in open(csv_file_name,'r'):\n",
    "        cols = re.split(',|\\n',line)\n",
    "        train_images.append(cols[0])\n",
    "        \n",
    "        # 3rd column is label and needs to be converted to int type\n",
    "        train_labels.append(int(cols[2]))\n",
    "        \n",
    "                            \n",
    "    input_queue = tf.train.slice_input_producer([train_images,train_labels],\n",
    "                                               num_epochs = num_epochs,shuffle = True)\n",
    "    \n",
    "    return input_queue\n",
    "\n",
    "def read_data(input_queue):\n",
    "    image_file = input_queue[0]\n",
    "    label = input_queue[1]\n",
    "    \n",
    "    image =  tf.image.decode_png(tf.read_file(image_file),channels=image_color)\n",
    "    \n",
    "    return image,label,image_file\n",
    "\n",
    "def read_data_batch(csv_file_name,batch_size=batch_size):\n",
    "    input_queue = get_input_queue(csv_file_name)\n",
    "    image,label,file_name= read_data(input_queue)\n",
    "    print(label)\n",
    "    image = tf.reshape(image,[image_size,image_size,image_color])\n",
    "    \n",
    "   # print(type(image2))\n",
    "    \n",
    "    # random image\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image,max_delta=0.5)\n",
    "    image = tf.image.random_contrast(image,lower=0.2,upper=2.0)\n",
    "    image = tf.image.random_hue(image,max_delta=0.08)\n",
    "    image = tf.image.random_saturation(image,lower=0.2,upper=2.0)\n",
    "\n",
    "    \n",
    "    batch_image,batch_label,batch_file = tf.train.batch([image,label,file_name],batch_size=batch_size)\n",
    "    #,enqueue_many=True)\n",
    "    batch_file = tf.reshape(batch_file,[batch_size,1])\n",
    "\n",
    "    batch_label_on_hot=tf.one_hot(tf.to_int64(batch_label),\n",
    "        num_classes, on_value=1.0, off_value=0.0)\n",
    "    print(batch_label_on_hot)\n",
    "    return batch_image,batch_label_on_hot,batch_file\n",
    "\n",
    "# convolutional network layer 1\n",
    "def conv1(input_data):\n",
    "    # layer 1 (convolutional layer)\n",
    "    conv1_filter_size = 3\n",
    "    conv1_layer_size = 16\n",
    "    stride1 = 1\n",
    "    \n",
    "    with tf.name_scope('conv_1'):\n",
    "        W_conv1 = tf.Variable(tf.truncated_normal(\n",
    "                        [conv1_filter_size,conv1_filter_size,image_color,conv1_layer_size],\n",
    "                                              stddev=0.1))\n",
    "        b1 = tf.Variable(tf.truncated_normal(\n",
    "                        [conv1_layer_size],stddev=0.1))\n",
    "        h_conv1 = tf.nn.conv2d(input_data,W_conv1,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv1_relu = tf.nn.relu(tf.add(h_conv1,b1))\n",
    "        h_conv1_maxpool = tf.nn.max_pool(h_conv1_relu\n",
    "                                        ,ksize=[1,2,2,1]\n",
    "                                        ,strides=[1,2,2,1],padding='SAME')\n",
    "        \n",
    "        \n",
    "    return h_conv1_maxpool\n",
    "\n",
    "# convolutional network layer 2\n",
    "def conv2(input_data):\n",
    "    conv2_filter_size = 3\n",
    "    conv2_layer_size = 32\n",
    "    stride2 = 1\n",
    "    \n",
    "    with tf.name_scope('conv_2'):\n",
    "        W_conv2 = tf.Variable(tf.truncated_normal(\n",
    "                        [conv2_filter_size,conv2_filter_size,conv1_layer_size,conv2_layer_size],\n",
    "                                              stddev=0.1))\n",
    "        b2 = tf.Variable(tf.truncated_normal(\n",
    "                        [conv2_layer_size],stddev=0.1))\n",
    "        h_conv2 = tf.nn.conv2d(input_data,W_conv2,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv2_relu = tf.nn.relu(tf.add(h_conv2,b2))\n",
    "        h_conv2_maxpool = tf.nn.max_pool(h_conv2_relu\n",
    "                                        ,ksize=[1,2,2,1]\n",
    "                                        ,strides=[1,2,2,1],padding='SAME')\n",
    "        \n",
    "        \n",
    "    return h_conv2_maxpool\n",
    "\n",
    "# convolutional network layer 3\n",
    "def conv3(input_data):\n",
    "    conv3_filter_size = 3\n",
    "    conv3_layer_size = 64\n",
    "    stride3 = 1\n",
    "    \n",
    "    print ('## FLAGS.stride1 ',stride1)\n",
    "    with tf.name_scope('conv_3'):\n",
    "        W_conv3 = tf.Variable(tf.truncated_normal(\n",
    "                        [conv3_filter_size,conv3_filter_size,conv2_layer_size,conv3_layer_size],\n",
    "                                              stddev=0.1))\n",
    "        b3 = tf.Variable(tf.truncated_normal(\n",
    "                        [conv3_layer_size],stddev=0.1))\n",
    "        h_conv3 = tf.nn.conv2d(input_data,W_conv3,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv3_relu = tf.nn.relu(tf.add(h_conv3,b3))\n",
    "        h_conv3_maxpool = tf.nn.max_pool(h_conv3_relu\n",
    "                                        ,ksize=[1,2,2,1]\n",
    "                                        ,strides=[1,2,2,1],padding='SAME')\n",
    "        \n",
    "        \n",
    "    return h_conv3_maxpool\n",
    "\n",
    "# convolutional network layer 3\n",
    "def conv4(input_data):\n",
    "    conv4_filter_size = 5\n",
    "    conv4_layer_size = 128\n",
    "    stride4 = 1\n",
    "    \n",
    "    with tf.name_scope('conv_4'):\n",
    "        W_conv4 = tf.Variable(tf.truncated_normal(\n",
    "                        [conv4_filter_size,conv4_filter_size,conv3_layer_size,conv4_layer_size],\n",
    "                                              stddev=0.1))\n",
    "        b4 = tf.Variable(tf.truncated_normal(\n",
    "                        [conv4_layer_size],stddev=0.1))\n",
    "        h_conv4 = tf.nn.conv2d(input_data,W_conv4,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv4_relu = tf.nn.relu(tf.add(h_conv4,b4))\n",
    "        h_conv4_maxpool = tf.nn.max_pool(h_conv4_relu\n",
    "                                        ,ksize=[1,2,2,1]\n",
    "                                        ,strides=[1,2,2,1],padding='SAME')\n",
    "        \n",
    "        \n",
    "    return h_conv4_maxpool\n",
    "\n",
    "# fully connected layer 1\n",
    "def fc1(input_data):\n",
    "    input_layer_size = 6*6*conv4_layer_size\n",
    "    fc1_layer_size = 256\n",
    "    \n",
    "    with tf.name_scope('fc_1'):\n",
    "        # 앞에서 입력받은 다차원 텐서를 fcc에 넣기 위해서 1차원으로 피는 작업\n",
    "        input_data_reshape = tf.reshape(input_data, [-1, input_layer_size])\n",
    "        W_fc1 = tf.Variable(tf.truncated_normal([input_layer_size,fc1_layer_size],stddev=0.1))\n",
    "        b_fc1 = tf.Variable(tf.truncated_normal(\n",
    "                        [fc1_layer_size],stddev=0.1))\n",
    "        h_fc1 = tf.add(tf.matmul(input_data_reshape,W_fc1) , b_fc1) # h_fc1 = input_data*W_fc1 + b_fc1\n",
    "        h_fc1_relu = tf.nn.relu(h_fc1)\n",
    "    \n",
    "    return h_fc1_relu\n",
    "    \n",
    "# fully connected layer 2\n",
    "def fc2(input_data):\n",
    "    fc2_layer_size = 256\n",
    "    \n",
    "    with tf.name_scope('fc_2'):\n",
    "        W_fc2 = tf.Variable(tf.truncated_normal([fc1_layer_size,fc2_layer_size],stddev=0.1))\n",
    "        b_fc2 = tf.Variable(tf.truncated_normal(\n",
    "                        [fc2_layer_size],stddev=0.1))\n",
    "        h_fc2 = tf.add(tf.matmul(input_data,W_fc2) , b_fc2) # h_fc1 = input_data*W_fc1 + b_fc1\n",
    "        h_fc2_relu = tf.nn.relu(h_fc2)\n",
    "    \n",
    "    return h_fc2_relu\n",
    "\n",
    "# final layer\n",
    "def final_out(input_data):\n",
    "\n",
    "    with tf.name_scope('final_out'):\n",
    "        W_fo = tf.Variable(tf.truncated_normal([fc2_layer_size,num_classes],stddev=0.1))\n",
    "        b_fo = tf.Variable(tf.truncated_normal(\n",
    "                        [num_classes],stddev=0.1))\n",
    "        h_fo = tf.add(tf.matmul(input_data,W_fo) , b_fo) # h_fc1 = input_data*W_fc1 + b_fc1\n",
    "        \n",
    "    # 최종 레이어에 softmax 함수는 적용하지 않았다. \n",
    "        \n",
    "    return h_fo\n",
    "\n",
    "# build cnn_graph\n",
    "def build_model(images,keep_prob):\n",
    "    # define CNN network graph\n",
    "    # output shape will be (*,48,48,16)\n",
    "    r_cnn1 = conv1(images) # convolutional layer 1\n",
    "    print (\"shape after cnn1 \",r_cnn1.get_shape())\n",
    "    \n",
    "    # output shape will be (*,24,24,32)\n",
    "    r_cnn2 = conv2(r_cnn1) # convolutional layer 2\n",
    "    print (\"shape after cnn2 :\",r_cnn2.get_shape() )\n",
    "    \n",
    "    # output shape will be (*,12,12,64)\n",
    "    r_cnn3 = conv3(r_cnn2) # convolutional layer 3\n",
    "    print (\"shape after cnn3 :\",r_cnn3.get_shape() )\n",
    "\n",
    "    # output shape will be (*,6,6,128)\n",
    "    r_cnn4 = conv4(r_cnn3) # convolutional layer 4\n",
    "    print (\"shape after cnn4 :\",r_cnn4.get_shape() )\n",
    "    \n",
    "    # fully connected layer 1\n",
    "    r_fc1 = fc1(r_cnn4)\n",
    "    print (\"shape after fc1 :\",r_fc1.get_shape() )\n",
    "\n",
    "    # fully connected layer2\n",
    "    r_fc2 = fc2(r_fc1)\n",
    "    print (\"shape after fc2 :\",r_fc2.get_shape() )\n",
    "    \n",
    "    ## drop out\n",
    "    # 참고 http://stackoverflow.com/questions/34597316/why-input-is-scaled-in-tf-nn-dropout-in-tensorflow\n",
    "    # 트레이닝시에는 keep_prob < 1.0 , Test 시에는 1.0으로 한다. \n",
    "    r_dropout = tf.nn.dropout(r_fc2,keep_prob)\n",
    "    print (\"shape after dropout :\",r_dropout.get_shape() ) \n",
    "    \n",
    "    # final layer\n",
    "    r_out = final_out(r_dropout)\n",
    "    print (\"shape after final layer :\",r_out.get_shape() )\n",
    "\n",
    "\n",
    "    return r_out \n",
    "\n",
    "\n",
    "images = tf.placeholder(tf.float32,[None,image_size,image_size,image_color])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout ratio\n",
    "\n",
    "prediction = tf.nn.softmax(build_model(images,keep_prob))\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, 'face_recognition/face_recog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import firebase_admin\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import urllib.request\n",
    "import os\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import db\n",
    "from subprocess import call\n",
    "import pyrebase\n",
    "from firebase import firebase\n",
    "\n",
    "# Firebase database 인증 및 앱 초기화\n",
    "cred = credentials.Certificate('myKey.json')\n",
    "firebase_admin.initialize_app(cred,{\n",
    "    'databaseURL' : \"https://aicctv-8f5ac.firebaseio.com/\"\n",
    "})\n",
    "\n",
    "config={\n",
    "    \"apiKey\": \"AIzaSyDAA2XR7x3iNsx9cQdhH6FVw8qY3Q3grFU\", # webkey\n",
    "    \"authDomain\": \"aicctv-8f5ac\", # projectID\n",
    "    \"databaseURL\": \"https://aicctv-8f5ac.firebaseio.com/\", \n",
    "    \"storageBucket\": \"aicctv-8f5ac.appspot.com\", # storageURL\n",
    "    \"serviceAccount\": \"myKey.json\"\n",
    "}\n",
    "fb=pyrebase.initialize_app(config)\n",
    "\n",
    "storage=fb.storage()\n",
    "database=fb.database()\n",
    "\n",
    "ID = \"00gpwls00\" # 이용자마다 다르게 코딩해야 하는 부분\n",
    "ref = db.reference(ID+\"/VideoLink\") #db 위치 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detect(link):\n",
    "    # 웹캠에서 영상을 읽어온다\n",
    "    cap = cv2.VideoCapture(link)\n",
    "    cap.set(3, 640) #WIDTH\n",
    "    cap.set(4, 480) #HEIGHT\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) #총 프레임 수\n",
    "    length_divide = int(length/30) #30초의 영상 프레임을 1초 단위로 확인\n",
    "    print(\"총 프레임 수 :\", length)\n",
    "    print(\"확인할 프레임 수 :\", length_divide)\n",
    "    global total\n",
    "    total=0\n",
    "    frequency =[0,0,0]\n",
    "    #얼굴 인식 캐스케이드 파일\n",
    "    face_cascade = cv2.CascadeClassifier('C:\\\\Users\\\\HEAJIN\\\\haarcascade_frontalface_alt.xml')\n",
    "\n",
    "    while(True):\n",
    "        #print('현재',cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        #마지막 프레임에 도달하면 break\n",
    "        if(int(cap.get(cv2.CAP_PROP_POS_FRAMES)) == length):\n",
    "            break\n",
    "            \n",
    "        #frame 별로 capture 한다\n",
    "        ret, frame = cap.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "        # 인식된 얼굴 갯수 및 프레임 번호를 출력\n",
    "        print(\"현재 프레임 :\", int(cap.get(cv2.CAP_PROP_POS_FRAMES)), \"/ 검출 face :\", len(faces))\n",
    "        if(int(cap.get(cv2.CAP_PROP_POS_FRAMES))%length_divide!=0):\n",
    "            continue\n",
    "            \n",
    "        # 얼굴이 검출되지 않은 경우 다음 프레임 확인\n",
    "        while (len(faces)==0 and (int(cap.get(cv2.CAP_PROP_POS_FRAMES))<length-1)):\n",
    "            ret, frame = cap.read()\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        # 인식된 얼굴에 사각형을 출력한다\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            cropped = frame[y:y + h , x:x + w ]\n",
    "            resize_img = cv2.resize(cropped, dsize=(96, 96), interpolation=cv2.INTER_AREA)\n",
    "            cv2.imwrite(\"captured.png\", resize_img)\n",
    "\n",
    "            tfimage = tf.image.decode_png(tf.read_file(\"captured.png\"),channels=3)\n",
    "            tfimage_value = tfimage.eval()\n",
    "            tfimages = []\n",
    "            tfimages.append(tfimage_value)\n",
    "            plt.imshow(tfimage_value) \n",
    "            plt.show()\n",
    "            #fd.close()\n",
    "\n",
    "            p_val = sess.run(prediction,feed_dict={images:tfimages,keep_prob:1.0})\n",
    "            global people\n",
    "            name_labels = []\n",
    "            for i in range (0, len(people)):\n",
    "                name_labels.append(people[i])\n",
    "         \n",
    "            i = 0\n",
    "          \n",
    "            max = -10\n",
    "            index=-1\n",
    "            for p in p_val[0]:\n",
    "                print('%s %f'% (name_labels[i],float(p)) )\n",
    "                if(p>max):\n",
    "                    max=p\n",
    "                    index=i\n",
    "                i = i + 1\n",
    "            total=total+1\n",
    "            frequency[index]=frequency[index]+1\n",
    "            print(\"\\n예측 결과 :\",name_labels[index], \"\\n\")\n",
    "            print(\"현재 프레임 :\",int(cap.get(cv2.CAP_PROP_POS_FRAMES)))\n",
    "         \n",
    "        \n",
    "        #화면에 출력한다\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    #print('\\n최종 비율:',frequency[0]*100.0/total,frequency[1]*100.0/total,frequency[2]*100.0/total)\n",
    "    max2=-1\n",
    "    index2=0\n",
    "    for k in range(len(frequency)):\n",
    "        if(max2<frequency[k]):\n",
    "            max2=frequency[k]\n",
    "            index2=k\n",
    "        k = k + 1\n",
    "    global check # 제일 최근 영상인지 확인하기 위한 변수\n",
    "    if(total==0 & check==True): # 빈 영상, 제일 최근 영상이 아닌 경우 -> 영상 삭제\n",
    "        global file_name\n",
    "        print(file_name+\" 영상 삭제\") # 얼굴 검출 안된 경우 -> 데베 & 스토리지 삭제\n",
    "        database.child(\"00gpwls00/VideoLink/\"+file_name).remove()\n",
    "        storage.delete(\"00gpwls00/Video/\"+file_name+\".mp4\")\n",
    "    elif(total==0 & check==False): # 빈 영상, 제일 최근 영상인 경우 -> 삭제하면 안됨\n",
    "        print(\"맨 마지막 영상입니다. 삭제하지 않습니다.\")\n",
    "    elif((frequency[index2]*100.0/total)>85):\n",
    "        print(\"최종 결과 :\", name_labels[index2]) # 아는 사람 -> 데베에 기록\n",
    "        database.child(\"00gpwls00/FaceResult/\"+file_name).set(name_labels[index2])\n",
    "    else:\n",
    "        print(\"최종 결과 : unknown\") # 모르는 사람 -> 데베에 기록 \n",
    "        database.child(\"00gpwls00/FaceResult/\"+file_name).set(\"unknown\")\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True: # 무한 반복문, 영상을 계속해서 다운로드 함\n",
    "    \n",
    "    # 영상 다운 및 처리 부분\n",
    "    dic=ref.get() # dict object\n",
    "    tmp_str=json.dumps(dic, ensure_ascii=False)[1:-1] # 한글을 포함해 string으로 변환 후 맨 앞과 뒤의 {} 제거\n",
    "    tmp=re.sub(r'\"*\"','', tmp_str) #  이름과 video linke만 남기고 나머지 문자 삭제\n",
    "\n",
    "    # 이용자 확인 부분\n",
    "    ref2=db.reference(ID+\"/PhotoLink\") #db 위치 지정\n",
    "    dic2=ref2.get() # dict object\n",
    "    tmp_str2=json.dumps(dic2, ensure_ascii=False)[1:-1] # 한글을 포함해 string으로 변환 후 맨 앞과 뒤의 {} 제거\n",
    "    tmp2=re.sub(r': {[^}]*}', '', tmp_str2) #  쉼표와 이름을 제외하고 모든 문자 삭제\n",
    "    global people\n",
    "    people=tmp2.split(\", \") # 등록된 사람들 이름만 추출 \n",
    "    \n",
    "    video_nonsp_name = tmp.split(\", \") # 동영상 이름만 추출 \n",
    "    video_name = []\n",
    "    video_url = []\n",
    "    for i in range(0, len(video_nonsp_name)):\n",
    "        video_name.append(video_nonsp_name[i].split(\": \")[0])\n",
    "        video_url.append(video_nonsp_name[i].split(\": \")[1])\n",
    "    \n",
    "    video_name = video_name[0:len(video_name)-2]\n",
    "    video_url = video_url[0:len(video_url)-2]\n",
    "    \n",
    "    video_ref=db.reference(ID+\"/VideoLink/\") \n",
    "    Download_ref=video_ref.child('Download') # 컴퓨터에 다운로드 된 마지막 영상의 이름\n",
    "    Update_ref=video_ref.child('Update') # 데이터베이스에 업로드 된 마지막 영상의 이름\n",
    "        \n",
    "    Download=Download_ref.get()\n",
    "    Update=Update_ref.get()\n",
    "        \n",
    "    # 다운받은 마지막 영상의 이름과 서버에 update된 마지막 영상의 이름이 다른 경우\n",
    "    while (Download != Update or Download=='0') : # 업데이트를 해야하는 경우 (다운로드가 필요한 경우)\n",
    "        outpath=\"C:/\"+ID+\"/video/\"\n",
    "        # 위의 outpath 경로가 존재하지 않을 경우, 경로를 생성함\n",
    "        if not os.path.isdir(outpath):\n",
    "            os.makedirs(outpath)\n",
    "        \n",
    "        count = 0\n",
    "        if(Download==0):\n",
    "            count = -1\n",
    "        else:\n",
    "            for i in range(0, len(video_name)):\n",
    "                if(video_name[i]==Download):\n",
    "                    count = i\n",
    "\n",
    "        for j in range (count+1, len(video_name)) : # 두 수의 차이만큼 반복 필요\n",
    "            global file_name # 위에서 삭제하기 위해 전역변수로 선언\n",
    "            file_name=video_name[j]\n",
    "            print(\"\\nfile name :\", file_name)\n",
    "            url=video_url[j]\n",
    "            urllib.request.urlretrieve(url, outpath+file_name+\".mp4\") # 다운로드\n",
    "            Download=file_name # 데이터베이스에 업데이트 하기 위해서 다운로드 받으며 체크\n",
    "            global total # 체크하기 위해 전역변수로 선언\n",
    "            global check # 맨 마지막 영상인지를 체크\n",
    "            if (Update==file_name):\n",
    "                check=False\n",
    "            else:\n",
    "                check=True\n",
    "            detect(outpath+file_name+\".mp4\")\n",
    "            if (total!=0): # 삭제 할 영상은 이름을 바꾸지 않음\n",
    "                Download_ref.set(Download) # 다운로드를 모두 완료한 후 변수값을 변경\n",
    "            \n",
    "    time.sleep(60); # 60초에 한번씩만 확인 및 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
